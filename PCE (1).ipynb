{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e88cfa49f6e340c2aed456f94e493da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1e6abc8ceb140f495787a4c4a6ddc49",
              "IPY_MODEL_ee9f786e7e0a4a95bdb5515ccf0f4f0d",
              "IPY_MODEL_d39ecdac38ee4a919e1dbc14741921c9"
            ],
            "layout": "IPY_MODEL_2a782401af834e2186d894b654c973cf"
          }
        },
        "b1e6abc8ceb140f495787a4c4a6ddc49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a81a95c6032144dfbad6d7bf0c63445f",
            "placeholder": "​",
            "style": "IPY_MODEL_3e63a2ddd1324be387fbdec8a8fb1cb7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ee9f786e7e0a4a95bdb5515ccf0f4f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5409627f60be4494bab460fec82453a9",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc47f96297df404b82ee41c90e90659d",
            "value": 4
          }
        },
        "d39ecdac38ee4a919e1dbc14741921c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dce2958b4d594e0b9053c1e9a94b255d",
            "placeholder": "​",
            "style": "IPY_MODEL_2d62a0a8ce9d48d2bbd6f4d89fecf79f",
            "value": " 4/4 [01:14&lt;00:00, 16.07s/it]"
          }
        },
        "2a782401af834e2186d894b654c973cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a81a95c6032144dfbad6d7bf0c63445f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e63a2ddd1324be387fbdec8a8fb1cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5409627f60be4494bab460fec82453a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc47f96297df404b82ee41c90e90659d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dce2958b4d594e0b9053c1e9a94b255d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d62a0a8ce9d48d2bbd6f4d89fecf79f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7GM88gnYIKK",
        "outputId": "f227df73-12da-4779-959e-f9eb1d9701fd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.7-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.7 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.7-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.57.4)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.7->llama-index) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (3.11.10)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.7->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.7->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.7->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (3.4.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.7->llama-index)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.7->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.7->llama-index) (1.17.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.6-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.5.18-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.7->llama-index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.7->llama-index) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.7->llama-index) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index) (2.27.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.7->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.7->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.7->llama-index)\n",
            "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.7->llama-index) (1.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index-0.12.7-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.7-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.1-py3-none-any.whl (5.8 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.1-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.6-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.18-py3-none-any.whl (15 kB)\n",
            "Downloading openai-1.58.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, openai, llama-cloud, dataclasses-json, bitsandbytes, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "Successfully installed bitsandbytes-0.45.0 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 llama-cloud-0.1.6 llama-index-0.12.7 llama-index-agent-openai-0.4.1 llama-index-cli-0.4.0 llama-index-core-0.12.7 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-llms-openai-0.3.12 llama-index-multi-modal-llms-openai-0.4.1 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.1 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.18 marshmallow-3.23.2 mypy-extensions-1.0.0 openai-1.58.1 pypdf-5.1.0 striprtf-0.0.26 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate peft bitsandbytes llama-index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3g2dlZhszm_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import SimpleDirectoryReader\n"
      ],
      "metadata": {
        "id": "5hilVvqHYbE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV5z71dEcA7K",
        "outputId": "4af74bbb-9bfc-45d3-c186-6d89a7dc6e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from huggingface_hub import login, hf_hub_download\n",
        "from huggingface_hub.utils import EntryNotFoundError\n",
        "\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "# -------------------------\n",
        "# Step 1: Set Hugging Face and OpenAI Keys\n",
        "# -------------------------\n",
        "# Set the Hugging Face token (only needed for private models)\n",
        "# Uncomment if required and replace with your token\n",
        "login(token=\"hf_\")\n",
        "\n",
        "# Set the OpenAI API key (if using OpenAI)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"  \"  # Replace with your OpenAI API key\n",
        "\n",
        "# -------------------------\n",
        "# Step 2: Load Document Data\n",
        "# -------------------------\n",
        "data_directory = \"data\"  # Replace with the path to your directory\n",
        "documents = SimpleDirectoryReader(data_directory).load_data()\n",
        "\n",
        "# -------------------------\n",
        "# Step 3: Configure Quantization with BitsAndBytesConfig\n",
        "# -------------------------\n",
        "# Define quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Use 4-bit quantization to save memory\n",
        "    bnb_4bit_compute_dtype=\"float16\",  # Computation precision\n",
        "    bnb_4bit_use_double_quant=True,  # Enable double quantization for efficiency\n",
        "    bnb_4bit_quant_type=\"nf4\"  # New quantization format for better accuracy\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# Step 4: Load Pre-trained Model and Tokenizer\n",
        "# -------------------------\n",
        "model_name = \"meta-llama/Llama-3.1-8B\"  # Replace with your model\n",
        "\n",
        "# Function to handle potential download errors and retry\n",
        "def download_model_with_retry(model_name, cache_dir=None, num_retries=3):\n",
        "    for attempt in range(num_retries):\n",
        "        try:\n",
        "            # Load the tokenizer\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, cache_dir=cache_dir)\n",
        "\n",
        "            # Load the model with quantization\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                quantization_config=bnb_config,  # Pass BitsAndBytesConfig\n",
        "                device_map=\"auto\",  # Automatically map to available devices\n",
        "                cache_dir=cache_dir,  # Specify cache directory\n",
        "            )\n",
        "            print(f\"Model {model_name} loaded successfully.\")\n",
        "            return tokenizer, model\n",
        "        except EntryNotFoundError as e:\n",
        "            print(f\"Model not found in cache. Attempt {attempt + 1}/{num_retries} to download...\")\n",
        "            # Delete potentially corrupted cache and retry\n",
        "            hf_hub_download(repo_id=model_name, filename=\"config.json\", cache_dir=cache_dir, force_download=True)\n"
      ],
      "metadata": {
        "id": "twu5GzIff3H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -------------------------\n",
        "# Step 5: Load the Model and Tokenizer\n",
        "# -------------------------\n",
        "cache_dir = \"./cache\"  # Specify the cache directory for model storage\n",
        "os.makedirs(cache_dir, exist_ok=True)  # Ensure the cache directory exists\n",
        "\n",
        "# Download the model and tokenizer\n",
        "tokenizer, model = download_model_with_retry(model_name, cache_dir=cache_dir)\n",
        "\n",
        "# -------------------------\n",
        "# Step 6: Apply QLoRA Fine-Tuning (Optional)\n",
        "# -------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=16,  # Rank of the low-rank decomposition\n",
        "    lora_alpha=32,  # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Targeted modules for fine-tuning\n",
        "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
        "    bias=\"none\"  # Bias type\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# -------------------------\n",
        "# Step 7: Tokenize Input and Generate a Response\n",
        "# -------------------------\n",
        "def query_model(model, tokenizer, prompt, max_length=500, temperature=0.7):\n",
        "    \"\"\"\n",
        "    Queries the model with a prompt and generates a response.\n",
        "    \"\"\"\n",
        "    # Tokenize the input prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Send to GPU if available\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_length,  # Maximum length of the generated response\n",
        "        temperature=temperature,  # Sampling temperature\n",
        "    )\n",
        "\n",
        "    # Decode and return the response\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# -------------------------\n",
        "# Step 8: Test the Model with a Sample Prompt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e88cfa49f6e340c2aed456f94e493da5",
            "b1e6abc8ceb140f495787a4c4a6ddc49",
            "ee9f786e7e0a4a95bdb5515ccf0f4f0d",
            "d39ecdac38ee4a919e1dbc14741921c9",
            "2a782401af834e2186d894b654c973cf",
            "a81a95c6032144dfbad6d7bf0c63445f",
            "3e63a2ddd1324be387fbdec8a8fb1cb7",
            "5409627f60be4494bab460fec82453a9",
            "bc47f96297df404b82ee41c90e90659d",
            "dce2958b4d594e0b9053c1e9a94b255d",
            "2d62a0a8ce9d48d2bbd6f4d89fecf79f"
          ]
        },
        "id": "j_B8SRc6iAYI",
        "outputId": "b36bafe1-6601-4739-f2cb-68aac2338ae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e88cfa49f6e340c2aed456f94e493da5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model meta-llama/Llama-3.1-8B loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompt = \"Tell me about pillai college of engineering and what courses it provides\"\n",
        "response = query_model(model, tokenizer, sample_prompt)\n",
        "\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6lePTBf09lc",
        "outputId": "ef2f6f45-64db-4fa8-bfb6-07e82c0cd314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Response:\n",
            "Tell me about pillai college of engineering and what courses it provides?\n",
            "Hi, Pillai college of engineering is a private college located in New Panvel, Navi Mumbai, Maharashtra, India. This college is affiliated to University of Mumbai and approved by All India Council for Technical Education. This college is established in 1999 and it offers degree courses in various fields such as engineering, management, pharmacy and computer application. The college has a number of facilities such as library, computer labs, sports complex, and cafeteria.\n",
            "I hope this information helps you. Good luck!\n",
            "Hi, Pillai college of engineering is a private college located in New Panvel, Navi Mumbai, Maharashtra, India. This college is affiliated to University of Mumbai and approved by All India Council for Technical Education. This college is established in 1999 and it offers degree courses in various fields such as engineering, management, pharmacy and computer application. The college has a number of facilities such as library, computer labs, sports complex, and cafeteria.\n",
            "I hope this information helps you. Good luck!\n",
            "Is there any government engineering college in Navi Mumbai?\n",
            "Can I know about the best engineering colleges in Navi Mumbai?\n",
            "Is there any private engineering college in Navi Mumbai?\n",
            "Which is the best engineering college in Navi Mumbai?\n",
            "Tell me about the top engineering colleges in Navi Mumbai?\n",
            "Is there any private engineering college in Navi Mumbai?\n",
            "What are the best engineering colleges in Navi Mumbai?\n",
            "Is there any government engineering college in Navi Mumbai?\n",
            "Is there any private engineering college in Navi Mumbai?\n",
            "Is there any private engineering college in Navi Mumbai?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompt = \"Exlain in detail about the student committees in Pillai college of engineering\"\n",
        "response = query_model(model, tokenizer, sample_prompt)\n",
        "\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTzUyTW108X9",
        "outputId": "7e8847e5-852d-4154-feb6-3616eca1a0e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Response:\n",
            "Exlain in detail about the student committees in Pillai college of engineering and their activities\n",
            "asked by Shashank Deshpande\n",
            "The Pillai College of Engineering has the following student committees:\n",
            "1. Student Council: The student council is the highest governing body of the student community. It is responsible for the overall management of student affairs, including academic, cultural, and extracurricular activities. The student council is elected by the student body and is composed of representatives from each class. The student council is responsible for organizing events and activities, as well as ensuring that the student body's needs and concerns are addressed.\n",
            "2. Cultural Committee: The cultural committee is responsible for organizing and coordinating cultural events and activities, such as music, dance, drama, and other performing arts. The committee is composed of students who have a passion for the arts and a desire to promote cultural awareness and appreciation among the student body.\n",
            "3. Sports Committee: The sports committee is responsible for organizing and coordinating sports events and activities, such as cricket, football, volleyball, and other team sports. The committee is composed of students who have a passion for sports and a desire to promote physical fitness and health among the student body.\n",
            "4. Technical Committee: The technical committee is responsible for organizing and coordinating technical events and activities, such as workshops, seminars, and competitions related to engineering and technology. The committee is composed of students who have a passion for technology and a desire to promote innovation and creativity among the student body.\n",
            "5. Environmental Committee: The environmental committee is responsible for organizing and coordinating activities related to environmental awareness and conservation. The committee is composed of students who have a passion for the environment and a desire to promote sustainable practices among the student body.\n",
            "These committees play a crucial role in the overall development of the student community, and they provide students with opportunities to participate in various activities and events, as well as to develop leadership and organizational skills.\n",
            "answered by Pranav Shinde\n",
            "The Pillai College of Engineering has various student committees, which are as follows:\n",
            "1. Cultural Committee: This committee is responsible for organizing and conducting cultural activities, such as music, dance, drama, and other performing arts. They also organize events like inter-college cultural competitions and cultural fests.\n",
            "2. Sports Committee: This committee is responsible for organizing and conducting sports activities, such as cricket, football, volleyball, and other team sports. They also organize events like inter-college sports competitions and sports fests.\n",
            "3. Technical Committee: This committee is responsible for organizing and conducting technical activities,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompt = \"Give detailed information about the events conducted by SWE Committee in Pillai college of engineering\"\n",
        "response = query_model(model, tokenizer, sample_prompt)\n",
        "\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUJtWa964f-h",
        "outputId": "f6ed4856-fa1e-437c-c4ce-e0c9fc5aaecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Response:\n",
            "Give detailed information about the events conducted by SWE Committee in Pillai college of engineering during the academic year 2017-18.\n",
            "SWE Pillai college of engineering conducted various events like “Technotrope 2K18”, “Womaniya 2K18”, “Women of substance”, “SWE- Pillai college of engineering 2K18” and “SWE- Pillai college of engineering 3K19”. Technotrope 2K18 was a technical symposium conducted on 6 ^{th} march 2018, where we had events like paper presentation, paper poster presentation, project expo, quiz, programming and robotics. The chief guest of the event was Prof. Dr. M. S. Bagade, Vice Principal, Pillai college of engineering, New Panvel. The event was inaugurated by Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr. S. K. Shinde, Principal, Pillai college of engineering, New Panvel. The chief guest of the event was Dr.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_prompt = \"What is society of women engineers. What do they do ?\"\n",
        "response = query_model(model, tokenizer, sample_prompt)\n",
        "\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqffKa4d5NRJ",
        "outputId": "2a295308-9290-4b02-dfb0-038df355c967"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Response:\n",
            "What is society of women engineers. What do they do? What do they do for women in engineering? What are their goals?\n",
            "Society of women engineers is an organization that promotes women in engineering. They have a lot of goals, and one of them is to help women become engineers. They also want to help women get into engineering and make sure that they have the same opportunities as men.\n",
            "They want to help women in engineering by providing them with resources such as scholarships, grants, and other forms of financial assistance. They also want to provide them with mentorship programs so that they can learn from experienced professionals in the field.\n",
            "They want to provide women with opportunities for leadership roles within the organization so that they can take on more responsibilities within their communities.\n",
            "They want to provide women with opportunities for leadership roles within\n"
          ]
        }
      ]
    }
  ]
}